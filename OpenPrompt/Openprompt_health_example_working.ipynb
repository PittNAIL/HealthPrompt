{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Bgm_jBuEbJb-"
   },
   "outputs": [],
   "source": [
    "!pip install -q openprompt==0.1.1 \\\n",
    "'torch>=1.9.0' \\\n",
    "'transformers>=4.10.0' \\\n",
    "sentencepiece==0.1.96 \\\n",
    "'scikit-learn>=0.24.2' \\\n",
    "'tqdm>=4.62.2' \\\n",
    "tensorboardX \\\n",
    "nltk \\\n",
    "yacs \\\n",
    "dill \\\n",
    "datasets \\\n",
    "rouge==1.0.0 \\\n",
    "scipy==1.4.1 \\\n",
    "fugashi \\\n",
    "ipadic \\\n",
    "unidic-lite"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "h40NMlRG5qWE"
   },
   "source": [
    "# BERT\n",
    "OpenPrompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "rHra-taOa7_R"
   },
   "outputs": [],
   "source": [
    "import openprompt.plms as plms\n",
    "from openprompt.plms.mlm import MLMTokenizerWrapper\n",
    "from transformers import BertConfig, BertForMaskedLM, BertTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "OZWrokM5cF8I"
   },
   "outputs": [],
   "source": [
    "plms._MODEL_CLASSES['bert'] = plms.ModelClass(**{\n",
    "    'config': BertConfig,\n",
    "    'tokenizer': BertTokenizer,\n",
    "    'model':BertForMaskedLM,\n",
    "    'wrapper': MLMTokenizerWrapper,\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "gsxMMn8pdQx2"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'bert': ModelClass(config=<class 'transformers.models.bert.configuration_bert.BertConfig'>, tokenizer=<class 'transformers.models.bert.tokenization_bert.BertTokenizer'>, model=<class 'transformers.models.bert.modeling_bert.BertForMaskedLM'>, wrapper=<class 'openprompt.plms.mlm.MLMTokenizerWrapper'>),\n",
       " 'roberta': ModelClass(config=<class 'transformers.models.roberta.configuration_roberta.RobertaConfig'>, tokenizer=<class 'transformers.models.roberta.tokenization_roberta.RobertaTokenizer'>, model=<class 'transformers.models.roberta.modeling_roberta.RobertaForMaskedLM'>, wrapper=<class 'openprompt.plms.mlm.MLMTokenizerWrapper'>),\n",
       " 'albert': ModelClass(config=<class 'transformers.models.albert.configuration_albert.AlbertConfig'>, tokenizer=<class 'transformers.models.albert.tokenization_albert.AlbertTokenizer'>, model=<class 'transformers.models.albert.modeling_albert.AlbertForMaskedLM'>, wrapper=<class 'openprompt.plms.mlm.MLMTokenizerWrapper'>),\n",
       " 'gpt': ModelClass(config=<class 'transformers.models.openai.configuration_openai.OpenAIGPTConfig'>, tokenizer=<class 'transformers.models.openai.tokenization_openai.OpenAIGPTTokenizer'>, model=<class 'transformers.models.openai.modeling_openai.OpenAIGPTLMHeadModel'>, wrapper=<class 'openprompt.plms.lm.LMTokenizerWrapper'>),\n",
       " 'gpt2': ModelClass(config=<class 'transformers.models.gpt2.configuration_gpt2.GPT2Config'>, tokenizer=<class 'transformers.models.gpt2.tokenization_gpt2.GPT2Tokenizer'>, model=<class 'transformers.models.gpt2.modeling_gpt2.GPT2LMHeadModel'>, wrapper=<class 'openprompt.plms.lm.LMTokenizerWrapper'>),\n",
       " 't5': ModelClass(config=<class 'transformers.models.t5.configuration_t5.T5Config'>, tokenizer=<class 'transformers.models.t5.tokenization_t5.T5Tokenizer'>, model=<class 'transformers.models.t5.modeling_t5.T5ForConditionalGeneration'>, wrapper=<class 'openprompt.plms.seq2seq.T5TokenizerWrapper'>),\n",
       " 't5-lm': ModelClass(config=<class 'transformers.models.t5.configuration_t5.T5Config'>, tokenizer=<class 'transformers.models.t5.tokenization_t5.T5Tokenizer'>, model=<class 'transformers.models.t5.modeling_t5.T5ForConditionalGeneration'>, wrapper=<class 'openprompt.plms.seq2seq.T5LMTokenizerWrapper'>)}"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "plms._MODEL_CLASSES"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lqqf6NKRKVgI"
   },
   "source": [
    "# Step 1: Define a task\n",
    "|"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "id": "YnhS5PiJJuPM"
   },
   "outputs": [],
   "source": [
    "from openprompt.data_utils import InputExample\n",
    "classes = [ \n",
    "    \"lung\",\n",
    "    \"brain\",\n",
    "    \"virus\"\n",
    "]\n",
    "\n",
    "dataset = [ \n",
    "      InputExample(\n",
    "#         guid = 0,\n",
    "        text_a = \"Asthma affects lungs  and can be hard to diagnose. The signs of asthma can seem like the signs of COPD, pneumonia, bronchitis, pulmonary embolism, anxiety, and heart disease.\", #lung\n",
    "    ),\n",
    "    InputExample(\n",
    "        guid = 1,\n",
    "        text_a = \"COVID-19 is caused by a coronavirus called SARS-CoV-2\", #virus\n",
    "    ),\n",
    "#     InputExample(\n",
    "#         guid = 2,\n",
    "#         text_a = \"When your brain is damaged, it can affect many different things, including your memory, your sensation, and even your personality. Brain disorders include any conditions or disabilities that affect your brain.\", #brain\n",
    "#     ),\n",
    "#     InputExample(\n",
    "#         guid = 3,\n",
    "#         text_a = \"Symptoms may appear 2-14 days after exposure to the virus\", #virus\n",
    "#     ),\n",
    "        InputExample(\n",
    "#         guid = 4,\n",
    "        text_a = \"\"\"Neurodegenerative diseases cause your brain and nerves to deteriorate over time. They can change your personality and cause confusion. They can also destroy your brain’s tissue and nerves.\n",
    "\n",
    "Some brain diseases, such as Alzheimer’s disease, may develop as you age. \"\"\", #brain\n",
    "    ),\n",
    "]\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DymjAiXFKYw7"
   },
   "source": [
    "# Step 2: Define a Pre-trained Language Models (PLMs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "id": "rFp3ya01J3nb"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForMaskedLM: ['cls.seq_relationship.bias', 'cls.seq_relationship.weight']\n",
      "- This IS expected if you are initializing BertForMaskedLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForMaskedLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoModel\n",
    "from openprompt.plms import load_plm\n",
    "plm, tokenizer, model_config, WrapperClass = load_plm(\"bert\", \"bert-base-uncased\")\n",
    "# plm, tokenizer, model_config, WrapperClass = load_plm(\"bert\", \"mrm8488/bioclinicalBERT-finetuned-covid-papers\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "id": "bM7D10XJg49q"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['i', 'think', 'this', 'drug', 'is', 'not', 'a', 'solution']"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.tokenize(\"I think this drug is not a solution\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zsI0hUZyKe4A"
   },
   "source": [
    "# Step 3: Define a Template.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "id": "HG7p1ETaKg4O"
   },
   "outputs": [],
   "source": [
    "from openprompt.prompts import ManualTemplate\n",
    "from openprompt.prompts import PtuningTemplate\n",
    "# template_text = '{\"placeholder\":\"text_a\"}: This effects {\"mask\"}'\n",
    "template_text= 'A {\"mask\"} disorder :  {\"placeholder\": \"text_a\"}'\n",
    "\n",
    "promptTemplate = ManualTemplate(\n",
    "    text = template_text,\n",
    "    tokenizer = tokenizer,\n",
    ")\n",
    "\n",
    "# promptTemplate = PtuningTemplate(model = plm, \n",
    "#                                  tokenizer = tokenizer, \n",
    "#                                  text = template_text, \n",
    "#                                  prompt_encoder_type = 'mlm'\n",
    "# )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hthNbpJCKh9R"
   },
   "source": [
    "# Step 4: Define a Verbalizer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "id": "QALc5AqJKl6w"
   },
   "outputs": [],
   "source": [
    "from openprompt.prompts import ManualVerbalizer\n",
    "promptVerbalizer = ManualVerbalizer(\n",
    "    classes = classes,\n",
    "    label_words = {\n",
    "        \"lung\": [\"chest\"],\n",
    "        \"brain\": [\"head\"],\n",
    "        \"virus\": [\"virus\"],\n",
    "    },\n",
    "    tokenizer = tokenizer,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WD8leL4PKpNM"
   },
   "source": [
    "# Step 5: Combine them into a PromptModel\n",
    "Given the task, now we have a PLM, a Template and a Verbalizer,  combine them into a PromptModel. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "id": "U-qTGso-Kqez"
   },
   "outputs": [],
   "source": [
    "from openprompt import PromptForClassification\n",
    "promptModel = PromptForClassification(\n",
    "    template = promptTemplate,\n",
    "    plm = plm,\n",
    "    verbalizer = promptVerbalizer\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4E7vuggeKuCI"
   },
   "source": [
    "# Step 6: Define a DataLoader\n",
    "A PromptDataLoader is basically a prompt version of pytorch Dataloader, which also includes a Tokenizer, a Template and a TokenizerWrapper."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "id": "AkrXsdLgKwh7"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "tokenizing: 3it [00:00, 405.48it/s]\n"
     ]
    }
   ],
   "source": [
    "from openprompt import PromptDataLoader\n",
    "data_loader = PromptDataLoader(\n",
    "    dataset = dataset,\n",
    "    tokenizer = tokenizer, \n",
    "    template = promptTemplate, \n",
    "    tokenizer_wrapper_class=WrapperClass,\n",
    "    max_seq_length=256, decoder_max_length=3, \n",
    "    batch_size=1,shuffle=False, teacher_forcing=False, predict_eos_token=False,\n",
    "    truncate_method=\"head\"\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rCQSvF5HK0Z3"
   },
   "source": [
    "# Step 7: Train and inference\n",
    "Done! We can conduct training and inference the same as other processes in Pytorch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "id": "YT4WwqpYK1Z0"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-0.0102, -5.0081, -5.6592]])\n",
      "lung\n",
      "tensor([[-4.7051, -6.2038, -0.0111]])\n",
      "virus\n",
      "tensor([[-3.2428, -0.0456, -5.1991]])\n",
      "brain\n"
     ]
    }
   ],
   "source": [
    "# making zero-shot inference using pretrained MLM with prompt\n",
    "import torch\n",
    "promptModel.eval()\n",
    "with torch.no_grad():\n",
    "    for batch in data_loader:\n",
    "        logits = promptModel(batch)\n",
    "        print(logits)\n",
    "        preds = torch.argmax(logits, dim = -1)\n",
    "        print(classes[preds])\n",
    "# predictions would be 1, 0 for classes 'positive', 'negative'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "RsY1V4xFQvIw"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "Openprompt-health example working.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
