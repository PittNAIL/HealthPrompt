{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "Bgm_jBuEbJb-"
   },
   "outputs": [],
   "source": [
    "!pip install -q openprompt==0.1.1 \\\n",
    "'torch>=1.9.0' \\\n",
    "'transformers>=4.10.0' \\\n",
    "sentencepiece==0.1.96 \\\n",
    "'scikit-learn>=0.24.2' \\\n",
    "'tqdm>=4.62.2' \\\n",
    "tensorboardX \\\n",
    "nltk \\\n",
    "yacs \\\n",
    "dill \\\n",
    "datasets \\\n",
    "rouge==1.0.0 \\\n",
    "scipy==1.4.1 \\\n",
    "fugashi \\\n",
    "ipadic \\\n",
    "unidic-lite"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "rHra-taOa7_R"
   },
   "outputs": [],
   "source": [
    "import openprompt.plms as plms\n",
    "from openprompt.plms.mlm import MLMTokenizerWrapper\n",
    "from transformers import BertConfig, BertForMaskedLM, BertTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "OZWrokM5cF8I"
   },
   "outputs": [],
   "source": [
    "plms._MODEL_CLASSES['bert'] = plms.ModelClass(**{\n",
    "    'config': BertConfig,\n",
    "    'tokenizer': BertTokenizer,\n",
    "    'model':BertForMaskedLM,\n",
    "    'wrapper': MLMTokenizerWrapper,\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "gsxMMn8pdQx2"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'bert': ModelClass(config=<class 'transformers.models.bert.configuration_bert.BertConfig'>, tokenizer=<class 'transformers.models.bert.tokenization_bert.BertTokenizer'>, model=<class 'transformers.models.bert.modeling_bert.BertForMaskedLM'>, wrapper=<class 'openprompt.plms.mlm.MLMTokenizerWrapper'>),\n",
       " 'roberta': ModelClass(config=<class 'transformers.models.roberta.configuration_roberta.RobertaConfig'>, tokenizer=<class 'transformers.models.roberta.tokenization_roberta.RobertaTokenizer'>, model=<class 'transformers.models.roberta.modeling_roberta.RobertaForMaskedLM'>, wrapper=<class 'openprompt.plms.mlm.MLMTokenizerWrapper'>),\n",
       " 'albert': ModelClass(config=<class 'transformers.models.albert.configuration_albert.AlbertConfig'>, tokenizer=<class 'transformers.models.albert.tokenization_albert.AlbertTokenizer'>, model=<class 'transformers.models.albert.modeling_albert.AlbertForMaskedLM'>, wrapper=<class 'openprompt.plms.mlm.MLMTokenizerWrapper'>),\n",
       " 'gpt': ModelClass(config=<class 'transformers.models.openai.configuration_openai.OpenAIGPTConfig'>, tokenizer=<class 'transformers.models.openai.tokenization_openai.OpenAIGPTTokenizer'>, model=<class 'transformers.models.openai.modeling_openai.OpenAIGPTLMHeadModel'>, wrapper=<class 'openprompt.plms.lm.LMTokenizerWrapper'>),\n",
       " 'gpt2': ModelClass(config=<class 'transformers.models.gpt2.configuration_gpt2.GPT2Config'>, tokenizer=<class 'transformers.models.gpt2.tokenization_gpt2.GPT2Tokenizer'>, model=<class 'transformers.models.gpt2.modeling_gpt2.GPT2LMHeadModel'>, wrapper=<class 'openprompt.plms.lm.LMTokenizerWrapper'>),\n",
       " 't5': ModelClass(config=<class 'transformers.models.t5.configuration_t5.T5Config'>, tokenizer=<class 'transformers.models.t5.tokenization_t5.T5Tokenizer'>, model=<class 'transformers.models.t5.modeling_t5.T5ForConditionalGeneration'>, wrapper=<class 'openprompt.plms.seq2seq.T5TokenizerWrapper'>),\n",
       " 't5-lm': ModelClass(config=<class 'transformers.models.t5.configuration_t5.T5Config'>, tokenizer=<class 'transformers.models.t5.tokenization_t5.T5Tokenizer'>, model=<class 'transformers.models.t5.modeling_t5.T5ForConditionalGeneration'>, wrapper=<class 'openprompt.plms.seq2seq.T5LMTokenizerWrapper'>)}"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "plms._MODEL_CLASSES"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lqqf6NKRKVgI"
   },
   "source": [
    "# Step 1: Define a task\n",
    "|"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "YnhS5PiJJuPM"
   },
   "outputs": [],
   "source": [
    "from openprompt.data_utils import InputExample\n",
    "classes = [ \n",
    "    \"Smoking\",\n",
    "    \"not Smoking\"\n",
    "]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DymjAiXFKYw7"
   },
   "source": [
    "# Step 2: Define a Pre-trained Language Models (PLMs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "rFp3ya01J3nb"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at emilyalsentzer/Bio_ClinicalBERT were not used when initializing BertForMaskedLM: ['cls.seq_relationship.weight', 'cls.seq_relationship.bias']\n",
      "- This IS expected if you are initializing BertForMaskedLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForMaskedLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoModel\n",
    "from openprompt.plms import load_plm\n",
    "# plm, tokenizer, model_config, WrapperClass = load_plm(\"bert\", \"bert-base-uncased\")\n",
    "plm, tokenizer, model_config, WrapperClass = load_plm(\"bert\", \"emilyalsentzer/Bio_ClinicalBERT\")\n",
    "# plm, tokenizer, model_config, WrapperClass = load_plm(\"bert\", \"mrm8488/bioclinicalBERT-finetuned-covid-papers\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "bM7D10XJg49q"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['i', 'think', 'this', 'drug', 'is', 'not', 'a', 'solution']"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.tokenize(\"I think this drug is not a solution\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zsI0hUZyKe4A"
   },
   "source": [
    "# Step 3: Define a Template.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "id": "HG7p1ETaKg4O"
   },
   "outputs": [],
   "source": [
    "from openprompt.prompts import ManualTemplate\n",
    "from openprompt.prompts import PtuningTemplate\n",
    "# template_text = '{\"placeholder\":\"text_a\"}: This effects {\"mask\"}'\n",
    "template_text= 'A {\"mask\"} disorder :  {\"placeholder\": \"text_a\"}'\n",
    "\n",
    "# promptTemplate = ManualTemplate(\n",
    "#     text = template_text,\n",
    "#     tokenizer = tokenizer,\n",
    "# )\n",
    "\n",
    "promptTemplate = PtuningTemplate(model = plm, \n",
    "                                 tokenizer = tokenizer, \n",
    "                                 text = template_text, \n",
    "                                 prompt_encoder_type = 'mlm'\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hthNbpJCKh9R"
   },
   "source": [
    "# Step 4: Define a Verbalizer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "id": "QALc5AqJKl6w"
   },
   "outputs": [],
   "source": [
    "from openprompt.prompts import ManualVerbalizer\n",
    "promptVerbalizer = ManualVerbalizer(\n",
    "    classes = classes,\n",
    "    label_words = {\n",
    "        \"Smoking\": [\"smoking\", \"tobacco\", \"cigarette\"],\n",
    "        \"not Smoking\": [\"healthy\"] #\"healthy\", \n",
    "    },\n",
    "    tokenizer = tokenizer,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WD8leL4PKpNM"
   },
   "source": [
    "# Step 5: Combine them into a PromptModel\n",
    "Given the task, now we have a PLM, a Template and a Verbalizer,  combine them into a PromptModel. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "id": "U-qTGso-Kqez"
   },
   "outputs": [],
   "source": [
    "from openprompt import PromptForClassification\n",
    "promptModel = PromptForClassification(\n",
    "    template = promptTemplate,\n",
    "    plm = plm,\n",
    "    verbalizer = promptVerbalizer\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4E7vuggeKuCI"
   },
   "source": [
    "# Step 6: Define a DataLoader\n",
    "A PromptDataLoader is basically a prompt version of pytorch Dataloader, which also includes a Tokenizer, a Template and a TokenizerWrapper."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "id": "AkrXsdLgKwh7"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "tokenizing: 3it [00:00, 928.42it/s]\n"
     ]
    }
   ],
   "source": [
    "from openprompt import PromptDataLoader\n",
    "data_loader = PromptDataLoader(\n",
    "    dataset = dataset,\n",
    "    tokenizer = tokenizer, \n",
    "    template = promptTemplate, \n",
    "    tokenizer_wrapper_class=WrapperClass,\n",
    "    max_seq_length=256, decoder_max_length=3, \n",
    "    batch_size=1,shuffle=False, teacher_forcing=False, predict_eos_token=False,\n",
    "    truncate_method=\"head\"\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rCQSvF5HK0Z3"
   },
   "source": [
    "# Step 7: Train and inference\n",
    "Done! We can conduct training and inference the same as other processes in Pytorch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "id": "YT4WwqpYK1Z0"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-3.0085, -6.5984]])\n",
      "Smoking\n",
      "tensor([[-2.7888, -8.5340]])\n",
      "Smoking\n",
      "tensor([[-6.2428, -0.0253]])\n",
      "not Smoking\n"
     ]
    }
   ],
   "source": [
    "# making zero-shot inference using pretrained MLM with prompt\n",
    "import torch\n",
    "promptModel.eval()\n",
    "with torch.no_grad():\n",
    "    for batch in data_loader:\n",
    "        logits = promptModel(batch)\n",
    "        print(logits)\n",
    "        preds = torch.argmax(logits, dim = -1)\n",
    "        \n",
    "        print(classes[preds])\n",
    "# predictions would be 1, 0 for classes 'positive', 'negative'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "RsY1V4xFQvIw"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "Openprompt-health example working.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
