{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Bgm_jBuEbJb-"
   },
   "outputs": [],
   "source": [
    "!pip install -q openprompt==0.1.1 \\\n",
    "'torch>=1.9.0' \\\n",
    "'transformers>=4.10.0' \\\n",
    "sentencepiece==0.1.96 \\\n",
    "'scikit-learn>=0.24.2' \\\n",
    "'tqdm>=4.62.2' \\\n",
    "tensorboardX \\\n",
    "nltk \\\n",
    "yacs \\\n",
    "dill \\\n",
    "datasets \\\n",
    "rouge==1.0.0 \\\n",
    "scipy==1.4.1 \\\n",
    "fugashi \\\n",
    "ipadic \\\n",
    "unidic-lite"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "h40NMlRG5qWE"
   },
   "source": [
    "# BERT\n",
    "OpenPrompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "rHra-taOa7_R"
   },
   "outputs": [],
   "source": [
    "import openprompt.plms as plms\n",
    "from openprompt.plms.mlm import MLMTokenizerWrapper\n",
    "from transformers import BertConfig, BertForMaskedLM, BertTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "OZWrokM5cF8I"
   },
   "outputs": [],
   "source": [
    "plms._MODEL_CLASSES['bert'] = plms.ModelClass(**{\n",
    "    'config': BertConfig,\n",
    "    'tokenizer': BertTokenizer,\n",
    "    'model':BertForMaskedLM,\n",
    "    'wrapper': MLMTokenizerWrapper,\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "gsxMMn8pdQx2"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'bert': ModelClass(config=<class 'transformers.models.bert.configuration_bert.BertConfig'>, tokenizer=<class 'transformers.models.bert.tokenization_bert.BertTokenizer'>, model=<class 'transformers.models.bert.modeling_bert.BertForMaskedLM'>, wrapper=<class 'openprompt.plms.mlm.MLMTokenizerWrapper'>),\n",
       " 'roberta': ModelClass(config=<class 'transformers.models.roberta.configuration_roberta.RobertaConfig'>, tokenizer=<class 'transformers.models.roberta.tokenization_roberta.RobertaTokenizer'>, model=<class 'transformers.models.roberta.modeling_roberta.RobertaForMaskedLM'>, wrapper=<class 'openprompt.plms.mlm.MLMTokenizerWrapper'>),\n",
       " 'albert': ModelClass(config=<class 'transformers.models.albert.configuration_albert.AlbertConfig'>, tokenizer=<class 'transformers.models.albert.tokenization_albert.AlbertTokenizer'>, model=<class 'transformers.models.albert.modeling_albert.AlbertForMaskedLM'>, wrapper=<class 'openprompt.plms.mlm.MLMTokenizerWrapper'>),\n",
       " 'gpt': ModelClass(config=<class 'transformers.models.openai.configuration_openai.OpenAIGPTConfig'>, tokenizer=<class 'transformers.models.openai.tokenization_openai.OpenAIGPTTokenizer'>, model=<class 'transformers.models.openai.modeling_openai.OpenAIGPTLMHeadModel'>, wrapper=<class 'openprompt.plms.lm.LMTokenizerWrapper'>),\n",
       " 'gpt2': ModelClass(config=<class 'transformers.models.gpt2.configuration_gpt2.GPT2Config'>, tokenizer=<class 'transformers.models.gpt2.tokenization_gpt2.GPT2Tokenizer'>, model=<class 'transformers.models.gpt2.modeling_gpt2.GPT2LMHeadModel'>, wrapper=<class 'openprompt.plms.lm.LMTokenizerWrapper'>),\n",
       " 't5': ModelClass(config=<class 'transformers.models.t5.configuration_t5.T5Config'>, tokenizer=<class 'transformers.models.t5.tokenization_t5.T5Tokenizer'>, model=<class 'transformers.models.t5.modeling_t5.T5ForConditionalGeneration'>, wrapper=<class 'openprompt.plms.seq2seq.T5TokenizerWrapper'>),\n",
       " 't5-lm': ModelClass(config=<class 'transformers.models.t5.configuration_t5.T5Config'>, tokenizer=<class 'transformers.models.t5.tokenization_t5.T5Tokenizer'>, model=<class 'transformers.models.t5.modeling_t5.T5ForConditionalGeneration'>, wrapper=<class 'openprompt.plms.seq2seq.T5LMTokenizerWrapper'>)}"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "plms._MODEL_CLASSES"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lqqf6NKRKVgI"
   },
   "source": [
    "# Step 1: Define a task\n",
    "|"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "YnhS5PiJJuPM"
   },
   "outputs": [],
   "source": [
    "from openprompt.data_utils import InputExample\n",
    "classes = [ \n",
    "    \"Obesity\",\n",
    "    \"not Obese\"\n",
    "]\n",
    "\n",
    "dataset = [ \n",
    "      InputExample(\n",
    "        guid = 0,\n",
    "        text_a = \"\"\"Abdomen was obese , soft , no palpable\n",
    "            masses , normal bowel sounds. Groin showed no hernias. Pulses were\n",
    "            significant for 1+ dopplerable femorals bilaterally , 2+\n",
    "            dopplerable dorsalis pedis bilaterally , 1+ dopplerable posterior\n",
    "            tibial on the right , 2+ dopplerable posterior tibial on the left\n",
    "            and 2+ dopplerable popliteals. Rectal exam was guaiac positive\n",
    "            with some pain and no visible lesions. ABI was .66 of the dorsalis\n",
    "            pedis on the right , .44 of the dorsalis pedis on the left; .61\n",
    "            posterior tibial on the right and .66 posterior tibial on the left.\"\"\", #obese\n",
    "    ),\n",
    "    InputExample(\n",
    "        guid = 1,\n",
    "        text_a = \"\"\"\"Her admission physical examination was significant for\n",
    "            temperature of 100.2 , blood pressure of 102/53 , and saturating\n",
    "            98% on 2 liters. The patient was mildly anxious. She was\n",
    "            normocephalic and atraumatic and had surgical pupils bilaterally.\n",
    "            Her neck was supple and her jugular venous pressure was 8 cm.\n",
    "            She had decreased breath sounds throughout and had fine scattered\n",
    "            rales throughout her lung fields. Heart was regular rate and\n",
    "            rhythm with normal S1 and S2 with a 2/6 systolic ejection murmur\n",
    "            at the right upper sternal border without any radiation. Her\n",
    "            abdomen was obese , soft , nontender , and nondistended with good\n",
    "            bowel sounds. Her extremities revealed no clubbing , cyanosis , or\n",
    "            edema. She did have a right arm fistula for hemodialysis with a\n",
    "            good thrill. Neurologically , she was alert and oriented x3\n",
    "            and had a steady gait with a walker.\"\"\", #obsese\n",
    "    ),\n",
    "#     InputExample(\n",
    "#         guid = 2,\n",
    "#         text_a = \"When your brain is damaged, it can affect many different things, including your memory, your sensation, and even your personality. Brain disorders include any conditions or disabilities that affect your brain.\", #brain\n",
    "#     ),\n",
    "#     InputExample(\n",
    "#         guid = 3,\n",
    "#         text_a = \"Symptoms may appear 2-14 days after exposure to the virus\", #virus\n",
    "#     ),\n",
    "        InputExample(\n",
    "        guid = 4,\n",
    "        text_a = \"\"\"The patient is healthy and is having a balanced diet.\"\"\", #brain\n",
    "    ),\n",
    "]\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DymjAiXFKYw7"
   },
   "source": [
    "# Step 2: Define a Pre-trained Language Models (PLMs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "rFp3ya01J3nb"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at emilyalsentzer/Bio_ClinicalBERT were not used when initializing BertForMaskedLM: ['cls.seq_relationship.bias', 'cls.seq_relationship.weight']\n",
      "- This IS expected if you are initializing BertForMaskedLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForMaskedLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoModel\n",
    "from openprompt.plms import load_plm\n",
    "# plm, tokenizer, model_config, WrapperClass = load_plm(\"bert\", \"bert-base-uncased\")\n",
    "plm, tokenizer, model_config, WrapperClass = load_plm(\"bert\", \"emilyalsentzer/Bio_ClinicalBERT\")\n",
    "# plm, tokenizer, model_config, WrapperClass = load_plm(\"bert\", \"mrm8488/bioclinicalBERT-finetuned-covid-papers\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "bM7D10XJg49q"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['i', 'think', 'this', 'drug', 'is', 'not', 'a', 'solution']"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.tokenize(\"I think this drug is not a solution\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zsI0hUZyKe4A"
   },
   "source": [
    "# Step 3: Define a Template.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "HG7p1ETaKg4O"
   },
   "outputs": [],
   "source": [
    "from openprompt.prompts import ManualTemplate\n",
    "from openprompt.prompts import PtuningTemplate\n",
    "# template_text = '{\"placeholder\":\"text_a\"}: This effects {\"mask\"}'\n",
    "template_text= 'A {\"mask\"} disorder :  {\"placeholder\": \"text_a\"}'\n",
    "\n",
    "# promptTemplate = ManualTemplate(\n",
    "#     text = template_text,\n",
    "#     tokenizer = tokenizer,\n",
    "# )\n",
    "\n",
    "promptTemplate = PtuningTemplate(model = plm, \n",
    "                                 tokenizer = tokenizer, \n",
    "                                 text = template_text, \n",
    "                                 prompt_encoder_type = 'mlm'\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hthNbpJCKh9R"
   },
   "source": [
    "# Step 4: Define a Verbalizer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "id": "QALc5AqJKl6w"
   },
   "outputs": [],
   "source": [
    "from openprompt.prompts import ManualVerbalizer\n",
    "promptVerbalizer = ManualVerbalizer(\n",
    "    classes = classes,\n",
    "    label_words = {\n",
    "        \"Obesity\": [\"obesity\", \"obese\", \"overweight\"],\n",
    "        \"not Obese\": [\"healthy\", \"proper diet\", \"underweight\"] #\"healthy\", \n",
    "    },\n",
    "    tokenizer = tokenizer,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WD8leL4PKpNM"
   },
   "source": [
    "# Step 5: Combine them into a PromptModel\n",
    "Given the task, now we have a PLM, a Template and a Verbalizer,  combine them into a PromptModel. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "id": "U-qTGso-Kqez"
   },
   "outputs": [],
   "source": [
    "from openprompt import PromptForClassification\n",
    "promptModel = PromptForClassification(\n",
    "    template = promptTemplate,\n",
    "    plm = plm,\n",
    "    verbalizer = promptVerbalizer\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4E7vuggeKuCI"
   },
   "source": [
    "# Step 6: Define a DataLoader\n",
    "A PromptDataLoader is basically a prompt version of pytorch Dataloader, which also includes a Tokenizer, a Template and a TokenizerWrapper."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "id": "AkrXsdLgKwh7"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "tokenizing: 3it [00:00, 334.13it/s]\n"
     ]
    }
   ],
   "source": [
    "from openprompt import PromptDataLoader\n",
    "data_loader = PromptDataLoader(\n",
    "    dataset = dataset,\n",
    "    tokenizer = tokenizer, \n",
    "    template = promptTemplate, \n",
    "    tokenizer_wrapper_class=WrapperClass,\n",
    "    max_seq_length=256, decoder_max_length=3, \n",
    "    batch_size=1,shuffle=False, teacher_forcing=False, predict_eos_token=False,\n",
    "    truncate_method=\"head\"\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rCQSvF5HK0Z3"
   },
   "source": [
    "# Step 7: Train and inference\n",
    "Done! We can conduct training and inference the same as other processes in Pytorch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "id": "YT4WwqpYK1Z0"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-1.9434, -3.5926]])\n",
      "Obesity\n",
      "tensor([[-2.5573, -2.5847]])\n",
      "Obesity\n",
      "tensor([[-6.8596, -4.7382]])\n",
      "not Obese\n"
     ]
    }
   ],
   "source": [
    "# making zero-shot inference using pretrained MLM with prompt\n",
    "import torch\n",
    "promptModel.eval()\n",
    "with torch.no_grad():\n",
    "    for batch in data_loader:\n",
    "        logits = promptModel(batch)\n",
    "        print(logits)\n",
    "        preds = torch.argmax(logits, dim = -1)\n",
    "        print(classes[preds])\n",
    "# predictions would be 1, 0 for classes 'positive', 'negative'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "RsY1V4xFQvIw"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "Openprompt-health example working.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
